# -*- coding: utf-8 -*-
"""ML_Finance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14kW-SQOCfMoQV_tyMqqYonJnrifx_k_g
"""

import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, silhouette_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import joblib # For saving/loading models
import io # For downloading files

# --- Page Configuration (Theme & Layout) ---
st.set_page_config(
    page_title="Interactive Financial ML",
    page_icon="ðŸ’¹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Theme Colors (Inspired by Behance Link) ---
PRIMARY_COLOR = "#1E3A8A" # Dark Blue
SECONDARY_COLOR = "#3B82F6" # Medium Blue
ACCENT_COLOR = "#F59E0B" # Amber/Gold
BACKGROUND_COLOR = "#000000"
TEXT_COLOR = "#FFFFFF" # Dark Gray

# --- Custom CSS for Theming (Optional but enhances look) ---
st.markdown(f"""
<style>
    /* Main background */
    .stApp {{
        background-color: {BACKGROUND_COLOR};
        color: {TEXT_COLOR};
    }}
    /* Sidebar */
    .css-1d391kg {{ /* Sidebar main */
        background-color: #F1F7B5; /* Slightly darker gray for contrast */
    }}
    /* Buttons */
    .stButton>button {{
        background-color: {SECONDARY_COLOR};
        color: white;
        border-radius: 8px;
        border: none;
        padding: 10px 20px;
        transition: background-color 0.3s ease;
    }}
    .stButton>button:hover {{
        background-color: {PRIMARY_COLOR};
    }}
    /* Special Accent Button (e.g., for final action) */
    .stButton.accent>button {{
        background-color: {ACCENT_COLOR};
        color: {TEXT_COLOR};
    }}
    .stButton.accent>button:hover {{
        background-color: #D97706; /* Darker accent */
    }}
    /* Titles and Headers */
    h1, h2, h3 {{
        color: {PRIMARY_COLOR};
    }}
    /* Dataframe styling */
     .dataframe {{ /* Might need more specific selector */
        border: 1px solid #D1D5DB;
        border-radius: 8px;
    }}
</style>
""", unsafe_allow_html=True)

# --- Helper Functions ---#

def download_file(data, filename, label, file_format='csv'):
    """Generates a download button for dataframes or models."""
    buffer = io.BytesIO()
    if file_format == 'csv' and isinstance(data, pd.DataFrame):
        data.to_csv(buffer, index=False)
        mime = 'text/csv'
    elif file_format == 'joblib':
        joblib.dump(data, buffer)
        mime = 'application/octet-stream'
    else:
        st.error("Unsupported download format.")
        return

    buffer.seek(0)
    st.download_button(
        label=f"ðŸ“¥ Download {label}",
        data=buffer,
        file_name=filename,
        mime=mime,
    )

def plot_feature_importance(model, feature_names):
    """Plots feature importance for linear models."""
    if hasattr(model, 'coef_'):
        # For Linear Regression and Logistic Regression with single output
        if model.coef_.ndim == 1:
             importance = model.coef_
        # For Logistic Regression with multiple classes (coef_ shape is (n_classes, n_features))
        # We'll take the absolute sum across classes for simplicity, or you could plot per class
        elif model.coef_.ndim == 2:
             importance = np.abs(model.coef_).sum(axis=0)
        else:
             st.warning("Could not determine feature importance structure.")
             return None

        importance_df = pd.DataFrame({'feature': feature_names, 'importance': importance})
        importance_df = importance_df.sort_values('importance', ascending=False).head(15) # Show top 15

        fig = px.bar(importance_df, x='importance', y='feature', orientation='h',
                     title="Top Feature Importances", color_discrete_sequence=[SECONDARY_COLOR])
        fig.update_layout(yaxis={'categoryorder':'total ascending'})
        return fig
    elif hasattr(model, 'feature_importances_'): # For tree-based models (if added later)
        importance = model.feature_importances_
        importance_df = pd.DataFrame({'feature': feature_names, 'importance': importance})
        importance_df = importance_df.sort_values('importance', ascending=False).head(15)
        fig = px.bar(importance_df, x='importance', y='feature', orientation='h',
                     title="Top Feature Importances", color_discrete_sequence=[SECONDARY_COLOR])
        fig.update_layout(yaxis={'categoryorder':'total ascending'})
        return fig
    else:
        st.info("Feature importance plotting not available for this model type (e.g., K-Means).")
        return None

def plot_clusters(df, features, cluster_labels, kmeans_model):
    """Plots K-Means clusters using the first two selected features."""
    if len(features) < 2:
        st.warning("Need at least two features selected to plot clusters.")
        return None

    fig = px.scatter(df, x=features[0], y=features[1], color=cluster_labels,
                     color_continuous_scale=px.colors.qualitative.Vivid,
                     title=f'K-Means Clustering (K={kmeans_model.n_clusters}) - Features: {features[0]} vs {features[1]}')

    # Add centroids
    centroids = kmeans_model.cluster_centers_
    # Ensure features selected for plotting exist in the scaled data used for centroids
    # This requires knowing the mapping from original feature names to scaled data columns (usually the same order)
    try:
        feature_indices = [features.index(f) for f in features[:2]] # Get indices of the first two features
        fig.add_trace(go.Scatter(x=centroids[:, feature_indices[0]],
                                 y=centroids[:, feature_indices[1]],
                                 mode='markers',
                                 marker=dict(color=ACCENT_COLOR, size=15, symbol='x'),
                                 name='Centroids'))
    except IndexError:
         st.warning("Could not map selected plot features to centroid dimensions.")
    except Exception as e:
         st.warning(f"Could not plot centroids: {e}")


    fig.update_layout(legend_title_text='Cluster')
    return fig


# --- Initialize Session State ---
# Use session state to store data and intermediate results across button clicks
if 'data' not in st.session_state:
    st.session_state.data = None
if 'data_source' not in st.session_state:
    st.session_state.data_source = None # 'upload' or 'yahoo'
if 'selected_model_type' not in st.session_state:
    st.session_state.selected_model_type = None
if 'preprocessed_data' not in st.session_state:
    st.session_state.preprocessed_data = None
if 'features' not in st.session_state:
    st.session_state.features = None
if 'target' not in st.session_state:
    st.session_state.target = None
if 'X_train' not in st.session_state:
    st.session_state.X_train = None
if 'X_test' not in st.session_state:
    st.session_state.X_test = None
if 'y_train' not in st.session_state:
    st.session_state.y_train = None
if 'y_test' not in st.session_state:
    st.session_state.y_test = None
if 'model' not in st.session_state:
    st.session_state.model = None
if 'predictions' not in st.session_state:
    st.session_state.predictions = None
if 'metrics' not in st.session_state:
    st.session_state.metrics = None
if 'feature_names' not in st.session_state:
     st.session_state.feature_names = None
if 'scaler' not in st.session_state: # Added to store scaler
     st.session_state.scaler = None
if 'X_scaled_kmeans' not in st.session_state: # Added for K-Means scaled data
     st.session_state.X_scaled_kmeans = None
if 'cluster_labels' not in st.session_state: # Added for K-Means labels
     st.session_state.cluster_labels = None


# --- Sidebar ---
with st.sidebar:
    st.image("https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExYjNlZzMwYmZ0b2JqZzFqZ3Q2b3JqdjRmeWZpM3U5c3N0b3Z0NnZzdyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o7bu7F84a834V43Di/giphy.gif", caption="Welcome to Financial ML!") # Finance GIF
    st.header("1. Load Data")

    data_load_option = st.radio("Choose Data Source:", ("Upload Kragle CSV", "Fetch Yahoo Finance"), key="data_source_radio")

    uploaded_file = None
    ticker = None
    period = '1y'
    interval = '1d'

    if data_load_option == "Upload Kragle CSV":
        uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
        if uploaded_file is not None:
             # Use a different key for the button to avoid conflict with yfinance button
             if st.button("Load Uploaded Data", key="load_csv_btn"):
                try:
                    df = pd.read_csv(uploaded_file)
                    st.session_state.data = df
                    st.session_state.data_source = 'upload'
                    # Reset downstream states when new data is loaded
                    st.session_state.preprocessed_data = None
                    st.session_state.features = None
                    st.session_state.target = None
                    st.session_state.X_train = None
                    st.session_state.X_test = None
                    st.session_state.y_train = None
                    st.session_state.y_test = None
                    st.session_state.model = None
                    st.session_state.predictions = None
                    st.session_state.metrics = None
                    st.session_state.X_scaled_kmeans = None
                    st.session_state.cluster_labels = None
                    st.success("CSV file loaded successfully!")
                    st.rerun() # Rerun to update the main panel view
                except Exception as e:
                    st.error(f"Error loading CSV: {e}")

    elif data_load_option == "Fetch Yahoo Finance":
        ticker = st.text_input("Enter Stock Ticker(s) (e.g., AAPL MSFT)", "AAPL")
        col1, col2 = st.columns(2)
        with col1:
            period = st.selectbox("Select Period:", ['1d','5d','1mo','3mo','6mo','1y','2y','5y','10y','ytd','max'], index=5) # Default 1y
        with col2:
            interval = st.selectbox("Select Interval:", ['1m','2m','5m','15m','30m','60m','90m','1h','1d','5d','1wk','1mo','3mo'], index=8) # Default 1d

        # Use a different key for the button
        if st.button("Fetch Yahoo Data", key="fetch_yahoo_btn"):
            if ticker:
                try:
                    tickers = ticker.split()
                    st.info(f"Fetching data for: {', '.join(tickers)}...") # Progress message
                    data = yf.download(tickers, period=period, interval=interval)

                    # **Improved Check:** yfinance might return dict on partial failure
                    if isinstance(data, dict):
                         st.error("Received unexpected data structure (possibly partial failure). Please check tickers.")
                         st.write(data) # Show the dictionary for debugging
                    elif data.empty:
                        st.warning(f"No data fetched for {ticker}. Check ticker symbol(s), period '{period}', interval '{interval}', and market hours.")
                    else:
                        # Process the DataFrame
                        data_df = data.copy() # Work with a copy

                        # If multiple tickers, the columns are multi-level. Flatten.
                        if isinstance(data_df.columns, pd.MultiIndex):
                             # Check if 'Date' or 'Datetime' is the index name
                             if data_df.index.name in ['Date', 'Datetime']:
                                  data_df = data_df.reset_index() # Make Date/Datetime a column if it's the index
                             data_df.columns = ['_'.join(col).strip().replace(' ', '_') for col in data_df.columns.values] # Flatten and clean names
                             # Find the primary date/datetime column after flattening
                             date_col = next((col for col in data_df.columns if col.lower() in ['date', 'datetime']), None)
                             # Drop rows where essential data (e.g., Close) might be missing for any ticker
                             close_cols = [col for col in data_df.columns if 'Close' in col and col != date_col] # Find all 'Close' columns
                             if close_cols:
                                  data_df = data_df.dropna(subset=close_cols, how='all') # Drop only if ALL close prices are NaN for a row
                        elif data_df.index.name in ['Date', 'Datetime']:
                             # Single ticker, reset index if Date/Datetime is index
                             data_df = data_df.reset_index()

                        if data_df.empty:
                             st.warning("Data became empty after processing (e.g., removing NaNs). Check fetched data range.")
                             st.session_state.data = None
                        else:
                             st.session_state.data = data_df
                             st.session_state.data_source = 'yahoo'
                             # Reset downstream states when new data is loaded
                             st.session_state.preprocessed_data = None
                             st.session_state.features = None
                             st.session_state.target = None
                             st.session_state.X_train = None
                             st.session_state.X_test = None
                             st.session_state.y_train = None
                             st.session_state.y_test = None
                             st.session_state.model = None
                             st.session_state.predictions = None
                             st.session_state.metrics = None
                             st.session_state.X_scaled_kmeans = None
                             st.session_state.cluster_labels = None
                             st.success(f"Yahoo Finance data for {ticker} fetched!")
                             st.rerun() # Rerun to update the main panel view

                except Exception as e:
                    st.error(f"Error fetching/processing Yahoo Finance data: {e}")
                    st.exception(e) # Show traceback for detailed debugging
            else:
                st.warning("Please enter at least one stock ticker.")

    st.divider()

    # --- Model Selection (Only if data is loaded) ---
    if st.session_state.data is not None:
        st.header("2. Select Model")
        model_options = ["Linear Regression", "Logistic Regression", "K-Means Clustering"]
        # Pre-select based on session state or default
        current_model_index = model_options.index(st.session_state.selected_model_type) if st.session_state.selected_model_type in model_options else 0

        model_type = st.selectbox(
            "Choose Machine Learning Model:",
            model_options,
            index=current_model_index,
            key="model_select"
        )
        # Update state only if selection changes to prevent unnecessary resets
        if model_type != st.session_state.selected_model_type:
             st.session_state.selected_model_type = model_type
             # Reset model-specific states if model type changes
             st.session_state.preprocessed_data = None
             st.session_state.features = None
             st.session_state.target = None
             st.session_state.X_train = None
             st.session_state.X_test = None
             st.session_state.y_train = None
             st.session_state.y_test = None
             st.session_state.model = None
             st.session_state.predictions = None
             st.session_state.metrics = None
             st.session_state.X_scaled_kmeans = None
             st.session_state.cluster_labels = None
             st.info(f"{model_type} selected. Please proceed through the steps below.")
             st.rerun() # Rerun to reflect change immediately

    else:
        st.info("Load data first to select a model.")


# --- Main Application Area ---
st.title("ðŸ’¹ Interactive Financial Machine Learning Pipeline")

if st.session_state.data is None:
    st.info("Welcome! Please load data using the sidebar to begin the analysis.")
    # You could add a more elaborate welcome message or image here
    # st.image("path/to/your/finance/welcome_image.png")

else:
    # Use columns for better layout
    col1, col2 = st.columns([3, 1]) # Give more space to dataframe
    with col1:
        st.subheader("Step 1: Data Overview")
        st.dataframe(st.session_state.data.head())
    with col2:
        st.metric("Data Shape", f"{st.session_state.data.shape[0]} rows, {st.session_state.data.shape[1]} cols")
        st.metric("Data Source", st.session_state.data_source.upper())


    st.divider()

    # --- Step 2: Preprocessing ---
    st.subheader("Step 2: Preprocessing")
    # Only show button if data is loaded
    if st.session_state.data is not None and st.session_state.preprocessed_data is None:
        if st.button("Run Preprocessing", key="preprocess_btn"):
            try:
                with st.spinner("Preprocessing data..."):
                    data_to_process = st.session_state.data.copy()

                    # --- Handle Missing Values ---
                    st.write("Missing Values Before Imputation:")
                    missing_vals = data_to_process.isnull().sum()
                    st.dataframe(missing_vals[missing_vals > 0].reset_index(name='count')) # Show only columns with missing values

                    # Simple Imputation (mean for numeric, most frequent for categorical)
                    numeric_cols = data_to_process.select_dtypes(include=np.number).columns
                    categorical_cols = data_to_process.select_dtypes(exclude=np.number).columns

                    imputed_numeric = False
                    if not numeric_cols.empty:
                        num_imputer = SimpleImputer(strategy='mean')
                        data_to_process[numeric_cols] = num_imputer.fit_transform(data_to_process[numeric_cols])
                        imputed_numeric = True

                    imputed_categorical = False
                    if not categorical_cols.empty:
                         # Avoid imputing date/time columns if they are object type
                         potential_datetime_cols = []
                         for col in categorical_cols:
                              try:
                                   # Attempt conversion, if > 50% success assume datetime-like
                                   pd.to_datetime(data_to_process[col], errors='coerce').notna().mean() > 0.5
                                   potential_datetime_cols.append(col)
                              except Exception:
                                   pass # Not datetime like

                         cols_to_impute = categorical_cols.difference(potential_datetime_cols)
                         if not cols_to_impute.empty:
                            cat_imputer = SimpleImputer(strategy='most_frequent')
                            data_to_process[cols_to_impute] = cat_imputer.fit_transform(data_to_process[cols_to_impute])
                            imputed_categorical = True

                    st.write("Missing Values After Imputation:")
                    missing_vals_after = data_to_process.isnull().sum()
                    st.dataframe(missing_vals_after[missing_vals_after > 0].reset_index(name='count')) # Show remaining missing if any
                    if not missing_vals_after.any():
                        st.info("No missing values remain after imputation.")


                    # --- Encoding Categorical Features (Example: Label Encoding) ---
                    # Be cautious with Label Encoding if categories are not ordinal.
                    # OneHotEncoding is often preferred but creates more columns.
                    encoders = {}
                    encoded_cols = []
                    for col in data_to_process.select_dtypes(include=['object', 'category']).columns:
                         # Avoid encoding potential date columns if they were objects
                         if col in potential_datetime_cols:
                              continue # Skip already identified datetime-like columns

                         # Also skip if column seems numeric despite being object (e.g., '123')
                         try:
                              pd.to_numeric(data_to_process[col], errors='raise')
                              continue
                         except (ValueError, TypeError):
                              # Proceed with encoding if not numeric or datetime-like
                              if data_to_process[col].nunique() < 100: # Heuristic: don't encode high-cardinality features
                                   le = LabelEncoder()
                                   data_to_process[col] = le.fit_transform(data_to_process[col])
                                   encoders[col] = le # Store encoder if needed later
                                   encoded_cols.append(col)
                              else:
                                   st.warning(f"Skipping encoding for high-cardinality categorical feature: '{col}' ({data_to_process[col].nunique()} unique values)")


                    st.session_state.preprocessed_data = data_to_process
                    st.success("Preprocessing complete (Imputation & Basic Encoding).")
                    st.dataframe(st.session_state.preprocessed_data.head())
                    st.rerun() # Rerun to move to next state

            except Exception as e:
                st.error(f"Error during preprocessing: {e}")
                st.exception(e)
    elif st.session_state.preprocessed_data is not None:
         st.success("Step 2: Preprocessing already completed.")
         st.dataframe(st.session_state.preprocessed_data.head())
    else:
         st.warning("Load data first to enable preprocessing.")


    st.divider()

    # --- Step 3: Feature Engineering / Selection ---
    st.subheader("Step 3: Feature Engineering & Selection")
    if st.session_state.preprocessed_data is not None:
        # Only show if features/target haven't been confirmed yet
        if st.session_state.features is None:
            potential_features = st.session_state.preprocessed_data.columns.tolist()
            # Try to exclude date/time columns automatically if they exist
            datetime_cols = st.session_state.preprocessed_data.select_dtypes(include=['datetime', 'datetime64[ns]', 'timedelta']).columns
            potential_features = [col for col in potential_features if col not in datetime_cols]
            numeric_features = st.session_state.preprocessed_data[potential_features].select_dtypes(include=np.number).columns.tolist()

            # --- Feature Selection ---
            # Default to numeric features if available, else all potential features except last
            default_selection = numeric_features if numeric_features else (potential_features[:-1] if len(potential_features) > 1 else potential_features)
            selected_features = st.multiselect(
                "Select Feature Columns (X):",
                potential_features,
                default=default_selection,
                key="feature_select_ms"
            )

            # --- Target Selection (for Supervised Learning) ---
            target_column = None
            if st.session_state.selected_model_type in ["Linear Regression", "Logistic Regression"]:
                potential_targets = [col for col in potential_features if col not in selected_features]
                # Try to guess a sensible default target (e.g., 'Close' for stock data)
                default_target_guess = None
                # Prioritize common financial targets
                common_targets = ['Close', 'Adj Close', 'Volume', 'Target', 'Label', 'Signal']
                for t in common_targets:
                     # Check for exact match or common variations (e.g., 'AAPL_Close')
                     matching_targets = [pt for pt in potential_targets if t == pt or pt.endswith(f'_{t}')]
                     if matching_targets:
                          default_target_guess = matching_targets[0]
                          break
                # Fallback to last potential target if no common one found
                if not default_target_guess and potential_targets:
                     default_target_guess = potential_targets[-1]

                target_index = potential_targets.index(default_target_guess) if default_target_guess in potential_targets else 0

                target_column = st.selectbox(
                    "Select Target Column (y):",
                    potential_targets,
                    index=target_index,
                    key="target_select_sb"
                )

                # Specific check for Logistic Regression Target
                if st.session_state.selected_model_type == "Logistic Regression":
                     if target_column:
                          n_unique = st.session_state.preprocessed_data[target_column].nunique()
                          if n_unique != 2:
                               st.warning(f"Logistic Regression typically requires a binary target (2 unique values). Column '{target_column}' has {n_unique} unique values.")
                               st.info("Consider engineering a binary target (e.g., Price Up/Down = 1/0) or choose a different model/target.")
                          else:
                               st.success(f"Binary target '{target_column}' selected for Logistic Regression.")


            if st.button("Confirm Features & Target", key="confirm_features_btn"):
                if not selected_features:
                    st.warning("Please select at least one feature.")
                elif st.session_state.selected_model_type in ["Linear Regression", "Logistic Regression"] and not target_column:
                    st.warning("Please select a target column for this model type.")
                else:
                    st.session_state.features = selected_features
                    st.session_state.target = target_column
                    st.session_state.feature_names = selected_features # Store for later use (importance plot)
                    st.success("Features and target confirmed.")
                    st.rerun() # Rerun to proceed
        else:
             # Show confirmed features/target
             st.success("Step 3: Features & Target already selected.")
             st.write("Selected Features (X):", st.session_state.features)
             if st.session_state.target:
                 st.write("Selected Target (y):", st.session_state.target)

    else:
        st.info("Run preprocessing first to select features.")

    st.divider()

    # --- Step 4: Train/Test Split (Supervised) or Scaling (Unsupervised) ---
    st.subheader(f"Step 4: {'Train/Test Split & Scale' if st.session_state.selected_model_type != 'K-Means Clustering' else 'Feature Scaling'}")
    if st.session_state.features:
        # Check if this step is already done
        step4_done = (st.session_state.X_train is not None) if st.session_state.selected_model_type != "K-Means Clustering" else (st.session_state.X_scaled_kmeans is not None)

        if not step4_done:
            if st.session_state.selected_model_type != "K-Means Clustering":
                # --- Train/Test Split & Scale ---
                test_size = st.slider("Select Test Set Size:", 0.1, 0.5, 0.2, 0.05, key="test_size_slider")
                random_state = st.number_input("Set Random State for Split:", value=42, key="random_state_split")

                if st.button("Split & Scale Data", key="split_scale_btn"):
                    try:
                        with st.spinner("Splitting and scaling data..."):
                            X = st.session_state.preprocessed_data[st.session_state.features]
                            y = st.session_state.preprocessed_data[st.session_state.target]

                            # --- Scaling Features (Important!) ---
                            # Ensure only numeric features are scaled
                            numeric_features_in_X = X.select_dtypes(include=np.number).columns
                            non_numeric_features_in_X = X.select_dtypes(exclude=np.number).columns

                            if not numeric_features_in_X.empty:
                                scaler = StandardScaler()
                                X_scaled_numeric = scaler.fit_transform(X[numeric_features_in_X])
                                X_scaled_numeric_df = pd.DataFrame(X_scaled_numeric, index=X.index, columns=numeric_features_in_X)
                                st.session_state.scaler = scaler # Store the scaler
                                # Combine scaled numeric with non-numeric if they exist
                                if not non_numeric_features_in_X.empty:
                                     X_processed = pd.concat([X_scaled_numeric_df, X[non_numeric_features_in_X]], axis=1)
                                else:
                                     X_processed = X_scaled_numeric_df
                            else:
                                 # No numeric features to scale
                                 X_processed = X
                                 st.info("No numeric features selected for scaling.")

                            # Reorder columns to original selection order
                            X_processed = X_processed[st.session_state.features]


                            X_train, X_test, y_train, y_test = train_test_split(
                                X_processed, y, test_size=test_size, random_state=random_state
                            )
                            st.session_state.X_train = X_train
                            st.session_state.X_test = X_test
                            st.session_state.y_train = y_train
                            st.session_state.y_test = y_test

                            st.success("Data split into training/testing sets and numeric features scaled.")
                            st.write(f"Training set shape: X {X_train.shape}, y {y_train.shape}")
                            st.write(f"Testing set shape: X {X_test.shape}, y {y_test.shape}")

                            # Visualize split
                            labels = ['Training Set', 'Testing Set']
                            sizes = [len(X_train), len(X_test)]
                            fig = px.pie(values=sizes, names=labels, title='Train/Test Split Distribution',
                                         color_discrete_sequence=[SECONDARY_COLOR, ACCENT_COLOR])
                            st.plotly_chart(fig, use_container_width=True)
                            st.rerun() # Proceed

                    except Exception as e:
                        st.error(f"Error during data splitting/scaling: {e}")
                        st.exception(e)

            else:
                # --- Feature Scaling for K-Means ---
                 if st.button("Scale Features for Clustering", key="scale_kmeans_btn"):
                     try:
                          with st.spinner("Scaling features..."):
                               X = st.session_state.preprocessed_data[st.session_state.features]
                               # Ensure only numeric features are scaled
                               numeric_features_in_X = X.select_dtypes(include=np.number).columns
                               if not numeric_features_in_X.empty:
                                    scaler = StandardScaler()
                                    X_scaled_numeric = scaler.fit_transform(X[numeric_features_in_X])
                                    # Store scaled data directly for K-Means (as numpy array)
                                    st.session_state.X_scaled_kmeans = X_scaled_numeric
                                    st.session_state.scaler = scaler # Store scaler
                                    st.success("Numeric features scaled successfully using StandardScaler.")
                                    st.write("Scaled Data Shape:", X_scaled_numeric.shape)
                                    # Store feature names corresponding to the scaled array columns
                                    st.session_state.scaled_feature_names_kmeans = numeric_features_in_X.tolist()
                               else:
                                    st.error("K-Means requires numeric features for scaling. None found in selection.")
                                    st.session_state.X_scaled_kmeans = None # Ensure it's None if no scaling happened

                               st.rerun() # Proceed
                     except Exception as e:
                          st.error(f"Error during feature scaling: {e}")
                          st.exception(e)
        else:
             # Step 4 is already done
             st.success(f"Step 4: {'Train/Test Split & Scale' if st.session_state.selected_model_type != 'K-Means Clustering' else 'Feature Scaling'} already completed.")
             if st.session_state.selected_model_type != "K-Means Clustering":
                  st.write(f"Training set shape: X {st.session_state.X_train.shape}, y {st.session_state.y_train.shape}")
                  st.write(f"Testing set shape: X {st.session_state.X_test.shape}, y {st.session_state.y_test.shape}")
             else:
                  st.write("Scaled Data Shape for K-Means:", st.session_state.X_scaled_kmeans.shape)


    else:
        st.info("Confirm features first.")

    st.divider()

    # --- Step 5: Model Training ---
    st.subheader("Step 5: Train Model")
    # Check if prerequisites are met based on model type
    prerequisites_met = False
    if st.session_state.selected_model_type != "K-Means Clustering":
         if st.session_state.X_train is not None and st.session_state.y_train is not None:
              prerequisites_met = True
    else: # K-Means
         if st.session_state.X_scaled_kmeans is not None:
              prerequisites_met = True

    if prerequisites_met:
        # Only show button if model not yet trained
        if st.session_state.model is None:
            model_params = {}
            if st.session_state.selected_model_type == "K-Means Clustering":
                n_clusters = st.slider("Select Number of Clusters (K):", 2, 15, 3, key="kmeans_k_slider")
                model_params['n_clusters'] = n_clusters
                # Add other K-Means params if needed (init, n_init, max_iter)

            # Add params for other models if desired (e.g., regularization strength for Logistic Regression)
            # Example for Logistic Regression:
            # if st.session_state.selected_model_type == "Logistic Regression":
            #     C = st.number_input("Regularization Strength (C):", 0.01, 10.0, 1.0, 0.01)
            #     model_params['C'] = C

            if st.button(f"Train {st.session_state.selected_model_type} Model", key="train_model_btn"):
                try:
                    with st.spinner(f"Training {st.session_state.selected_model_type} model..."):
                        model = None
                        if st.session_state.selected_model_type == "Linear Regression":
                            model = LinearRegression()
                            # Ensure X_train is numeric (should be after scaling)
                            X_train_numeric = st.session_state.X_train.select_dtypes(include=np.number)
                            if X_train_numeric.shape[1] == 0:
                                 st.error("Linear Regression requires numeric features. None found in training data.")
                                 st.stop()
                            model.fit(X_train_numeric, st.session_state.y_train)
                            # Store names of features actually used in model
                            st.session_state.model_feature_names = X_train_numeric.columns.tolist()

                        elif st.session_state.selected_model_type == "Logistic Regression":
                            # Add solver='liblinear' for potential small datasets or binary classification
                            model = LogisticRegression(random_state=42, solver='liblinear', **model_params)
                            # Ensure target is suitable before fitting
                            if st.session_state.y_train.nunique() > 2:
                                 st.error("Cannot train Logistic Regression: Target variable is not binary.")
                                 st.stop() # Stop execution here
                            # Ensure X_train is numeric
                            X_train_numeric = st.session_state.X_train.select_dtypes(include=np.number)
                            if X_train_numeric.shape[1] == 0:
                                 st.error("Logistic Regression requires numeric features. None found in training data.")
                                 st.stop()
                            model.fit(X_train_numeric, st.session_state.y_train)
                            # Store names of features actually used in model
                            st.session_state.model_feature_names = X_train_numeric.columns.tolist()

                        elif st.session_state.selected_model_type == "K-Means Clustering":
                            model = KMeans(n_clusters=model_params['n_clusters'], random_state=42, n_init=10) # n_init=10 is recommended over 'auto'
                            model.fit(st.session_state.X_scaled_kmeans) # Fit on scaled numeric data
                            # Store feature names used for clustering (from scaling step)
                            st.session_state.model_feature_names = st.session_state.scaled_feature_names_kmeans


                        st.session_state.model = model
                        st.success(f"{st.session_state.selected_model_type} model trained successfully!")
                        # Optionally display model parameters: st.write(model.get_params())
                        st.rerun() # Proceed

                except Exception as e:
                    st.error(f"Error during model training: {e}")
                    st.exception(e)
        else:
            # Model already trained
            st.success(f"Step 5: {st.session_state.selected_model_type} Model already trained.")
            # Optionally display parameters of the trained model
            # st.write("Trained Model Parameters:")
            # st.write(st.session_state.model.get_params())

    else:
        st.info(f"Complete previous steps ({'Split & Scale Data' if st.session_state.selected_model_type != 'K-Means Clustering' else 'Scale Features'}) first.")

    st.divider()

    # --- Step 6: Evaluation ---
    st.subheader("Step 6: Evaluate Model")
    if st.session_state.model is not None:
         # Only show button if evaluation not yet done
         if st.session_state.metrics is None:
            if st.button("Evaluate Trained Model", key="evaluate_model_btn"):
                try:
                    with st.spinner("Evaluating model..."):
                        metrics = {}
                        # Ensure X_test is numeric for prediction
                        if st.session_state.selected_model_type != "K-Means Clustering":
                             X_test_numeric = st.session_state.X_test.select_dtypes(include=np.number)
                             if X_test_numeric.shape[1] != len(st.session_state.model_feature_names):
                                  st.error(f"Mismatch between features used for training ({len(st.session_state.model_feature_names)}) and numeric features in test set ({X_test_numeric.shape[1]}). Check preprocessing/scaling steps.")
                                  st.stop()
                             # Ensure column order matches training
                             X_test_numeric = X_test_numeric[st.session_state.model_feature_names]


                        if st.session_state.selected_model_type == "Linear Regression":
                            if st.session_state.X_test is not None:
                                predictions = st.session_state.model.predict(X_test_numeric)
                                st.session_state.predictions = predictions
                                metrics['MSE'] = mean_squared_error(st.session_state.y_test, predictions)
                                metrics['R2 Score'] = r2_score(st.session_state.y_test, predictions)
                                st.session_state.metrics = metrics

                                st.write("Evaluation Metrics:")
                                col1, col2 = st.columns(2)
                                col1.metric("Mean Squared Error (MSE)", f"{metrics['MSE']:.4f}")
                                col2.metric("R-squared (R2) Score", f"{metrics['R2 Score']:.4f}")
                            else:
                                 st.warning("Test data not available for evaluation.")

                        elif st.session_state.selected_model_type == "Logistic Regression":
                             if st.session_state.X_test is not None:
                                predictions = st.session_state.model.predict(X_test_numeric)
                                pred_proba = st.session_state.model.predict_proba(X_test_numeric) # Get probabilities
                                st.session_state.predictions = predictions
                                st.session_state.pred_proba = pred_proba # Store probabilities

                                metrics['Accuracy'] = accuracy_score(st.session_state.y_test, predictions)
                                metrics['Confusion Matrix'] = confusion_matrix(st.session_state.y_test, predictions).tolist() # Store as list for state
                                st.session_state.metrics = metrics

                                st.write("Evaluation Metrics:")
                                st.metric("Accuracy Score", f"{metrics['Accuracy']:.4f}")

                                # Display Confusion Matrix using Plotly heatmap
                                cm = metrics['Confusion Matrix']
                                fig_cm = px.imshow(cm, text_auto=True, aspect="auto",
                                                   labels=dict(x="Predicted Label", y="True Label", color="Count"),
                                                   x=[str(c) for c in st.session_state.model.classes_], # Ensure labels are strings
                                                   y=[str(c) for c in st.session_state.model.classes_],
                                                   title="Confusion Matrix", color_continuous_scale=px.colors.sequential.Blues)
                                st.plotly_chart(fig_cm, use_container_width=True)
                             else:
                                  st.warning("Test data not available for evaluation.")

                        elif st.session_state.selected_model_type == "K-Means Clustering":
                            # Evaluation for K-Means (using Silhouette Score on the data it was trained on)
                            labels = st.session_state.model.labels_
                            st.session_state.cluster_labels = labels # Store labels for visualization
                            # Check if more than 1 cluster exists before calculating silhouette score
                            if len(np.unique(labels)) > 1:
                                 metrics['Silhouette Score'] = silhouette_score(st.session_state.X_scaled_kmeans, labels)
                            else:
                                 metrics['Silhouette Score'] = None # Not defined for 1 cluster
                                 st.warning("Silhouette Score cannot be calculated for a single cluster (K=1).")

                            metrics['Inertia (WCSS)'] = st.session_state.model.inertia_ # Within-cluster sum of squares
                            st.session_state.metrics = metrics

                            st.write("Clustering Evaluation Metrics:")
                            col1, col2 = st.columns(2)
                            if metrics['Silhouette Score'] is not None:
                                 col1.metric("Silhouette Score", f"{metrics['Silhouette Score']:.4f}")
                            else:
                                 col1.metric("Silhouette Score", "N/A")
                            col2.metric("Inertia (WCSS)", f"{metrics['Inertia (WCSS)']:.2f}")
                            st.info("Silhouette Score ranges from -1 to 1. Higher is generally better (more dense, well-separated clusters). Inertia measures within-cluster variance (lower is better).")

                        st.success("Model evaluation complete.")
                        st.rerun() # Proceed

                except Exception as e:
                    st.error(f"Error during model evaluation: {e}")
                    st.exception(e)
         else:
             # Evaluation already done
             st.success("Step 6: Model evaluation already completed.")
             # Display metrics again
             metrics = st.session_state.metrics
             st.write("Evaluation Metrics:")
             if st.session_state.selected_model_type == "Linear Regression":
                 col1, col2 = st.columns(2)
                 col1.metric("Mean Squared Error (MSE)", f"{metrics['MSE']:.4f}")
                 col2.metric("R-squared (R2) Score", f"{metrics['R2 Score']:.4f}")
             elif st.session_state.selected_model_type == "Logistic Regression":
                  st.metric("Accuracy Score", f"{metrics['Accuracy']:.4f}")
                  # Display Confusion Matrix again
                  cm = metrics['Confusion Matrix']
                  fig_cm = px.imshow(cm, text_auto=True, aspect="auto",
                                     labels=dict(x="Predicted Label", y="True Label", color="Count"),
                                     x=[str(c) for c in st.session_state.model.classes_],
                                     y=[str(c) for c in st.session_state.model.classes_],
                                     title="Confusion Matrix", color_continuous_scale=px.colors.sequential.Blues)
                  st.plotly_chart(fig_cm, use_container_width=True)
             elif st.session_state.selected_model_type == "K-Means Clustering":
                  col1, col2 = st.columns(2)
                  if metrics['Silhouette Score'] is not None:
                       col1.metric("Silhouette Score", f"{metrics['Silhouette Score']:.4f}")
                  else:
                       col1.metric("Silhouette Score", "N/A")
                  col2.metric("Inertia (WCSS)", f"{metrics['Inertia (WCSS)']:.2f}")

    else:
        st.info("Train a model first.")

    st.divider()

    # --- Step 7: Results Visualization & Download ---
    st.subheader("Step 7: Visualize Results & Download")
    if st.session_state.metrics is not None: # Check if evaluation has run

        # --- Visualization ---
        st.write("Result Visualizations:")
        try:
            if st.session_state.selected_model_type == "Linear Regression" and st.session_state.predictions is not None:
                # Scatter plot of Actual vs Predicted
                results_df = pd.DataFrame({'Actual': st.session_state.y_test, 'Predicted': st.session_state.predictions})
                fig_pred = px.scatter(results_df, x='Actual', y='Predicted',
                                      title='Actual vs. Predicted Values (Linear Regression)',
                                      labels={'Actual': 'Actual Values', 'Predicted': 'Predicted Values'},
                                      trendline='ols', trendline_color_override=ACCENT_COLOR,
                                      hover_data=results_df.columns) # Show both values on hover
                fig_pred.add_shape(type='line', x0=results_df['Actual'].min(), y0=results_df['Actual'].min(),
                                   x1=results_df['Actual'].max(), y1=results_df['Actual'].max(),
                                   line=dict(color='gray', dash='dash')) # Add y=x line
                st.plotly_chart(fig_pred, use_container_width=True)

                # Feature Importance Plot
                if 'model_feature_names' in st.session_state and st.session_state.model_feature_names:
                    fig_imp = plot_feature_importance(st.session_state.model, st.session_state.model_feature_names)
                    if fig_imp:
                         st.plotly_chart(fig_imp, use_container_width=True)

            elif st.session_state.selected_model_type == "Logistic Regression":
                 # Confusion matrix was already plotted in evaluation
                 st.info("Confusion Matrix plotted during evaluation step.")
                 # Feature Importance Plot
                 if 'model_feature_names' in st.session_state and st.session_state.model_feature_names:
                     fig_imp = plot_feature_importance(st.session_state.model, st.session_state.model_feature_names)
                     if fig_imp:
                          st.plotly_chart(fig_imp, use_container_width=True)
                 # Optional: Plot ROC Curve or Precision-Recall Curve if needed


            elif st.session_state.selected_model_type == "K-Means Clustering":
                 # Plot clusters
                 if 'cluster_labels' in st.session_state and st.session_state.features:
                      # Use original preprocessed data for plotting, add cluster labels
                      df_plot = st.session_state.preprocessed_data.copy()
                      # Ensure cluster labels align with the data (KMeans fits on all provided data)
                      if len(df_plot) == len(st.session_state.cluster_labels):
                           df_plot['Cluster'] = st.session_state.cluster_labels.astype(str) # Use string for discrete colors
                           # Plot using the first two ORIGINAL features selected by the user
                           plot_features = st.session_state.features[:2]
                           if len(plot_features) >= 2:
                                fig_cluster = px.scatter(df_plot, x=plot_features[0], y=plot_features[1],
                                                         color='Cluster', # Color by cluster label
                                                         title=f'K-Means Clustering (K={st.session_state.model.n_clusters}) - Features: {plot_features[0]} vs {plot_features[1]}',
                                                         color_discrete_sequence=px.colors.qualitative.Vivid) # Use qualitative colors
                                st.plotly_chart(fig_cluster, use_container_width=True)
                           else:
                                st.warning("Need at least two features selected in Step 3 to generate cluster plot.")
                      else:
                           st.warning("Mismatch between data length and cluster labels. Cannot plot clusters accurately.")
                 else:
                      st.warning("Cluster labels or features not available for plotting.")

        except Exception as e:
            st.error(f"Error generating visualizations: {e}")
            st.exception(e)


        # --- Download Options ---
        st.write("Download Artifacts:")
        col1, col2 = st.columns(2)

        with col1:
             # Download Model
             if st.session_state.model:
                  model_filename = f"{st.session_state.selected_model_type.replace(' ', '_').lower()}_model.joblib"
                  try:
                       download_file(st.session_state.model, model_filename, "Trained Model", file_format='joblib')
                  except Exception as e:
                       st.error(f"Error preparing model for download: {e}")

        with col2:
             # Download Predictions/Results (if applicable)
             try:
                 if st.session_state.predictions is not None and st.session_state.selected_model_type != "K-Means Clustering":
                     # Combine X_test, y_test, and predictions
                     results_df = st.session_state.X_test.copy()
                     results_df['Actual_Target'] = st.session_state.y_test
                     results_df['Predicted_Target'] = st.session_state.predictions
                     if st.session_state.selected_model_type == "Logistic Regression" and 'pred_proba' in st.session_state:
                          # Add prediction probabilities for logistic regression
                          for i, class_label in enumerate(st.session_state.model.classes_):
                               results_df[f'Probability_{class_label}'] = st.session_state.pred_proba[:, i]

                     download_file(results_df, "test_predictions.csv", "Test Set Predictions", file_format='csv')

                 elif 'cluster_labels' in st.session_state and st.session_state.selected_model_type == "K-Means Clustering":
                      # Add cluster labels to the original preprocessed data
                      results_df = st.session_state.preprocessed_data.copy()
                      if len(results_df) == len(st.session_state.cluster_labels):
                           results_df['Cluster'] = st.session_state.cluster_labels
                           download_file(results_df, "clustered_data.csv", "Data with Clusters", file_format='csv')
                      else:
                           st.warning("Could not align cluster labels with data for download.")

             except Exception as e:
                  st.error(f"Error preparing results for download: {e}")


        st.success("Analysis complete! ðŸŽ‰ You can explore the visualizations and download the results.")
        st.image("https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExemN3amVkbTU5ODN3cjR4bHp6N3pza2p4MXQ4dmp4YzBmYm0zMndzciZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/DjolzwbnW9pGAOatK9/giphy.gif", caption="Analysis Finished!")


    elif st.session_state.model is not None: # Model trained but not evaluated
         st.info("Evaluate the model first to see results and visualizations.")
    else:
         st.info("Train and evaluate a model first.")

# --- Add Footer or additional info ---
st.markdown("---")
st.markdown("Built with Streamlit | Financial Data Analysis")